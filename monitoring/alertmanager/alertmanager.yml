global:
  resolve_timeout: 5m
  slack_api_url: 'https://hooks.slack.com/services/REPLACE_WITH_ACTUAL_SLACK_WEBHOOK_URL'
  smtp_smarthost: 'smtp.example.com:587'
  smtp_from: 'alertmanager@example.com'
  smtp_auth_username: 'alertmanager'
  smtp_auth_password: 'replace_with_actual_password'  # In production, use secret management
  smtp_require_tls: true
  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'
  opsgenie_api_url: 'https://api.opsgenie.com/'
  victorops_api_url: 'https://alert.victorops.com/integrations/generic/20131114/alert/'

# The directory from which notification templates are read
templates:
  - '/etc/alertmanager/template/*.tmpl'

# The root route on which each incoming alert enters
route:
  # The labels by which incoming alerts are grouped together
  group_by: ['alertname', 'job', 'severity']
  
  # When a new group of alerts is created by an incoming alert
  group_wait: 30s
  
  # When the first notification was sent, wait 'group_interval' to send a batch of new alerts
  group_interval: 5m
  
  # If an alert has successfully been sent, wait 'repeat_interval' to resend
  repeat_interval: 4h
  
  # Default receiver
  receiver: 'team-mlops'
  
  # Child routes
  routes:
  - match:
      severity: critical
    receiver: 'team-mlops-pager'
    repeat_interval: 1h
    continue: true
  
  - match:
      severity: warning
    receiver: 'team-mlops'
    repeat_interval: 3h
    continue: true
  
  - match_re:
      service: ^(gliner-api|model-service)$
    receiver: 'team-mlops'
    routes:
    - match:
        severity: critical
      receiver: 'team-mlops-pager'
  
  # Special route for database alerts
  - match_re:
      service: ^(postgres|mongodb|redis)$
    receiver: 'db-team'
    group_wait: 10s
    group_interval: 1m

# Inhibition rules allow to mute a set of alerts given that another alert is
# firing. We use this to mute warnings and infos when the same alertname
# is critical.
inhibit_rules:
- source_match:
    severity: 'critical'
  target_match:
    severity: 'warning'
  equal: ['alertname', 'instance']

- source_match:
    severity: 'warning'
  target_match:
    severity: 'info'
  equal: ['alertname', 'instance']

receivers:
- name: 'team-mlops'
  slack_configs:
  - channel: '#mlops-alerts'
    send_resolved: true
    title: "{{ .GroupLabels.alertname }} - {{ .Status | toUpper }}"
    title_link: 'https://grafana.example.com/d/gliner/gliner-model-monitoring'
    text: >-
      {{ range .Alerts }}
        *Alert:* {{ .Annotations.summary }}
        *Description:* {{ .Annotations.description }}
        *Severity:* {{ .Labels.severity }}
        *Instance:* {{ .Labels.instance }}
        {{ if .Annotations.runbook_url }}*Runbook:* {{ .Annotations.runbook_url }}{{ end }}
      {{ end }}
    icon_emoji: '{{ if eq .Status "firing" }}:fire:{{ else }}:ok_hand:{{ end }}'
    actions:
    - type: button
      text: 'View in Grafana'
      url: 'https://grafana.example.com/d/gliner/gliner-model-monitoring'
    - type: button
      text: 'View Runbook'
      url: 'https://wiki.example.com/runbooks/{{ .GroupLabels.alertname }}'
  
  email_configs:
  - to: 'mlops-team@example.com'
    send_resolved: true
    headers:
      subject: "{{ .GroupLabels.alertname }} - {{ .Status | toUpper }}"

- name: 'team-mlops-pager'
  pagerduty_configs:
  - service_key: 'replace_with_pagerduty_service_key'
    send_resolved: true
    description: >-
      [{{ .Status | toUpper }}] {{ .GroupLabels.alertname }} - {{ .CommonAnnotations.summary }}
    client: 'AlertManager'
    client_url: 'https://alertmanager.example.com'
    details:
      firing: '{{ template "pagerduty.default.instances" .Alerts.Firing }}'
      resolved: '{{ template "pagerduty.default.instances" .Alerts.Resolved }}'
      num_firing: '{{ .Alerts.Firing | len }}'
  
  slack_configs:
  - channel: '#mlops-critical'
    send_resolved: true
    title: "ðŸš¨ CRITICAL ALERT: {{ .GroupLabels.alertname }} - {{ .Status | toUpper }}"
    title_link: 'https://grafana.example.com/d/gliner/gliner-model-monitoring'
    text: >-
      {{ range .Alerts }}
        *Alert:* {{ .Annotations.summary }}
        *Description:* {{ .Annotations.description }}
        *Severity:* {{ .Labels.severity }}
        *Instance:* {{ .Labels.instance }}
        {{ if .Annotations.runbook_url }}*Runbook:* {{ .Annotations.runbook_url }}{{ end }}
      {{ end }}
    icon_emoji: ':rotating_light:'
    actions:
    - type: button
      text: 'Acknowledge'
      url: 'https://pagerduty.example.com/incidents/{{ if .CommonLabels.incident_id }}{{ .CommonLabels.incident_id }}{{ else }}unknown{{ end }}'
    - type: button
      text: 'View in Grafana'
      url: 'https://grafana.example.com/d/gliner/gliner-model-monitoring'
  
  email_configs:
  - to: 'oncall-mlops@example.com'
    send_resolved: true
    require_tls: true
    headers:
      subject: "ðŸš¨ CRITICAL ALERT: {{ .GroupLabels.alertname }} - {{ .Status | toUpper }}"

- name: 'db-team'
  slack_configs:
  - channel: '#db-alerts'
    send_resolved: true
    title: "Database Alert: {{ .GroupLabels.alertname }} - {{ .Status | toUpper }}"
    text: >-
      {{ range .Alerts }}
        *Alert:* {{ .Annotations.summary }}
        *Description:* {{ .Annotations.description }}
        *Severity:* {{ .Labels.severity }}
        *Instance:* {{ .Labels.instance }}
        {{ if .Annotations.runbook_url }}*Runbook:* {{ .Annotations.runbook_url }}{{ end }}
      {{ end }}
  
  email_configs:
  - to: 'db-team@example.com'
    send_resolved: true